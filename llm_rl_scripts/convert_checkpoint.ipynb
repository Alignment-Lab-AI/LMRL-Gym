{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import UnpicklingError\n",
    "from charset_normalizer import from_bytes\n",
    "import torch\n",
    "from transformers.modeling_flax_pytorch_utils import load_flax_checkpoint_in_pytorch_model, load_flax_weights_in_pytorch_model\n",
    "from transformers import T5ForConditionalGeneration, FlaxT5ForConditionalGeneration, AutoTokenizer\n",
    "from JaxSeq.bucket_manager import open_with_bucket as open\n",
    "from JaxSeq.models.T5.load import load_params as t5_load_params, ModelLoadMode as T5ModelLoadMode\n",
    "from JaxSeq.models.T5.interface import T5Inference\n",
    "from JaxSeq.utils import jsonl_stream, convert_path, load_mesh, get_dtype\n",
    "from JaxSeq.train import eval_loss\n",
    "from IPython import embed\n",
    "import os\n",
    "import jax\n",
    "import json\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from JaxSeq.data import MaskIterableDataset\n",
    "from JaxSeq.utils import BlockingStrategy, Padding, Truncation, get_weight_decay_mask, MapIterable, FileOpenIterable\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "flax_params, flax_model = t5_load_params(\n",
    "            model_load_mode=T5ModelLoadMode.PARAMS,\n",
    "            model_load_path=model_load_path,\n",
    "            tokenizer=\"google/flan-t5-xl\",\n",
    "            mesh=load_mesh((1, 1, -1), ('dp', 'fsdp', 'mp')),\n",
    "            model_dtype=get_dtype(use_fp16=True),\n",
    "            params_dtype=get_dtype(use_fp16=True)\n",
    "        )\n",
    "\n",
    "def load_t5_pytorch_model(params):\n",
    "    pt_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", architectures=[\"T5ForConditionalGeneration\"])\n",
    "\n",
    "    start_indices = (0, 0)\n",
    "    params[\"lm_head\"][\"kernel\"] = jax.lax.dynamic_slice(params[\"lm_head\"][\"kernel\"], start_indices, pt_model.state_dict()[\"lm_head.weight\"].T.shape)\n",
    "    params[\"shared\"][\"embedding\"] = jax.lax.dynamic_slice(params[\"shared\"][\"embedding\"], start_indices, pt_model.state_dict()[\"shared.weight\"].shape)\n",
    "\n",
    "    print(\"encoder size: \", pt_model.state_dict()[\"encoder.embed_tokens.weight\"].shape)\n",
    "    print(\"decoder size: \", pt_model.state_dict()[\"decoder.embed_tokens.weight\"].shape)\n",
    "\n",
    "    print(\"shared embedding size: \", params[\"shared\"][\"embedding\"].shape)\n",
    "    print(\"lm_head kernel size: \", params[\"lm_head\"][\"kernel\"].shape)\n",
    "\n",
    "    new_model = load_flax_weights_in_pytorch_model(pt_model, params)\n",
    "\n",
    "    # if changing to shared embedding \n",
    "    # new_model.state_dict()[\"encoder.embed_tokens.weight\"] = new_model.state_dict()[\"shared.weight\"]\n",
    "    # new_model.state_dict()[\"decoder.embed_tokens.weight\"] = new_model.state_dict()[\"shared.weight\"]\n",
    "    # if changing to language model head\n",
    "    # new_model.state_dict()[\"encoder.embed_tokens.weight\"] = new_model.state_dict()[\"lm_head.weight\"].T\n",
    "    # new_model.state_dict()[\"decoder.embed_tokens.weight\"] = new_model.state_dict()[\"lm_head.weight\"].T\n",
    "\n",
    "    new_model.state_dict()[\"encoder.embed_tokens.weight\"] = torch.zeros_like(new_model.state_dict()[\"encoder.embed_tokens.weight\"])\n",
    "    new_model.state_dict()[\"decoder.embed_tokens.weight\"] = torch.zeros_like(new_model.state_dict()[\"decoder.embed_tokens.weight\"])\n",
    "    return new_model\n",
    "\n",
    "pt_model = load_t5_pytorch_model(flax_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
